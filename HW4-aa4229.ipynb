{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "train_data = pd.read_csv('reddit_200k_train.csv',encoding='latin1')\n",
    "train_data = train_data[['body','REMOVED']]\n",
    "test_data = pd.read_csv('reddit_200k_test.csv',encoding='latin1')\n",
    "test_data = test_data[['body','REMOVED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>REMOVED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday: Drug companies stock dives on good new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i learned that all hybrids are unfertile i won...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  REMOVED\n",
       "0  I've always been taught it emerged from the ea...    False\n",
       "1  As an ECE, my first feeling as \"HEY THAT'S NOT...     True\n",
       "2  Monday: Drug companies stock dives on good new...     True\n",
       "3  i learned that all hybrids are unfertile i won...    False\n",
       "4  Well i was wanting to get wasted tonight.  Not...    False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167529"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REMOVED</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>102791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>64738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           body\n",
       "REMOVED        \n",
       "False    102791\n",
       "True      64738"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('REMOVED').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data clearly is imbalanced and we will under sample it before performing the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55843"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1  Bag of Words and simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a baseline model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    train_X = train_data[['body']]\n",
    "    train_y = train_data[['REMOVED']]    \n",
    "    return train_X,train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is highly skewed, we are undersampling the data to make it balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under sample the data\n",
    "train_X,train_y = get_all_data()\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "train_X_subsample, train_y_subsample = rus.fit_sample(\n",
    "    train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129476, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_subsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count vectorizer\n",
    "countvect = CountVectorizer()\n",
    "X = countvect.fit_transform(train_X_subsample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------test_accuracy------\n",
      "0.6890851700496398\n",
      "-------test_average_precision------\n",
      "0.7051205088440811\n",
      "-------test_f1------\n",
      "0.7112509023778023\n",
      "-------recall------\n",
      "0.7658715080867582\n"
     ]
    }
   ],
   "source": [
    "# test the base line model\n",
    "scores = cross_validate(LogisticRegression(),\n",
    "                        X, train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is skewed and imbalanced, the scoring metric selected here is F1 score. There are also other metrics I will be testing like precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model fordifferent feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We will remove stop word for all the cases following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to find the best parameters for n gram approach. We will tune the following parameters<br/>\n",
    "1. min_df <br/>\n",
    "2. ngram_range <br/>\n",
    "3. C<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search cv to get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'lr']\n",
      "parameters:\n",
      "{'vect__min_df': (5, 10), 'vect__ngram_range': ((1, 2), (1, 1)), 'lr__C': (0.1, 0.05)}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.691\n",
      "Best parameters set:\n",
      "\tlr__C: 0.05\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# run grid search for parameter tuning\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('lr',LogisticRegression(penalty='l2'))\n",
    "    \n",
    "])\n",
    "parameters = {\n",
    "    'vect__min_df': (5,10),\n",
    "    'vect__ngram_range': ((1, 2),(1,1)),\n",
    "    'lr__C':(0.1,0.05)\n",
    "}\n",
    "scoring = {'auc': 'roc_auc', 'accuracy':'accuracy', 'average_precision': 'average_precision', \n",
    "           'precision':'precision', 'recall':'recall','f1':'f1'}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1,scoring=scoring,refit='f1')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "grid_search.fit(train_X_subsample.ravel(), train_y_subsample)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now performing cross validation on best parameters to get model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance with best parameters\n",
      "-------test_accuracy------\n",
      "0.6921592335082319\n",
      "-------test_average_precision------\n",
      "0.7149158896517699\n",
      "-------test_f1------\n",
      "0.7245863939142969\n",
      "-------recall------\n",
      "0.8098798065514549\n"
     ]
    }
   ],
   "source": [
    "# run cross validation to test the model\n",
    "\n",
    "pipeline = make_pipeline(CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5),LogisticRegression(C=0.05,penalty='l2'))\n",
    "    \n",
    "scores = cross_validate(pipeline,\n",
    "                        train_X_subsample.ravel(), train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Model Performance with best parameters\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that recall increased significantly and precision and f1 score scores have increased slightly from the base line model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Approach is to introduce a tf-idf count vectorizer. We will remove the stop words from the data and also introduce a L2 penalty on the data.  \n",
    "\n",
    "We will tune parameters following parameters <br/>\n",
    "1. min_df\n",
    "2. C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search to find best parameters for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfid', 'lr']\n",
      "parameters:\n",
      "{'tfid__min_df': (5, 10), 'lr__C': (0.1, 0.2, 0.05)}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.694\n",
      "Best parameters set:\n",
      "\tlr__C: 0.2\n",
      "\ttfid__min_df: 5\n"
     ]
    }
   ],
   "source": [
    "# run grid search to tune the parameters\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfid',  TfidfVectorizer(stop_words='english')),\n",
    "    ('lr',LogisticRegression(penalty='l2'))\n",
    "])\n",
    "parameters = {\n",
    "    'tfid__min_df': (5,10),\n",
    "    'lr__C':(0.1,0.2,0.05)\n",
    "}\n",
    "scoring = {'auc': 'roc_auc', 'accuracy':'accuracy', 'average_precision': 'average_precision', \n",
    "           'precision':'precision', 'recall':'recall','f1':'f1'}\n",
    "grid_search_tf_idf = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1,scoring=scoring,refit='f1')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "grid_search_tf_idf.fit(train_X_subsample.ravel(), train_y_subsample)\n",
    "print(\"Best score: %0.3f\" % grid_search_tf_idf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_tf_idf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run cross-validation on best parameters obtained from grid search for tf-idf above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf model Performance with best parameters\n",
      "-------test_accuracy------\n",
      "0.6938584080068454\n",
      "-------test_average_precision------\n",
      "0.7286779195419226\n",
      "-------test_f1------\n",
      "0.7107047816199041\n",
      "-------recall------\n",
      "0.7520776119193578\n"
     ]
    }
   ],
   "source": [
    "# run cross validation to test the model\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(stop_words='english',min_df=5),LogisticRegression(C=0.2,penalty='l2'))\n",
    "    \n",
    "scores = cross_validate(pipeline,\n",
    "                        train_X_subsample.ravel(), train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Tf-idf model Performance with best parameters\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tf-idf without ngram reduced the recall whereas precision and f1 score remained the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again run grid search CV to tune a different set of paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfid', 'lr']\n",
      "parameters:\n",
      "{'tfid__min_df': (5, 10), 'lr__penalty': ('l1', 'l2'), 'lr__C': (0.1, 0.2, 0.05, 0.3, 0.6)}\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.696\n",
      "Best parameters set:\n",
      "\tlr__C: 0.6\n",
      "\tlr__penalty: 'l2'\n",
      "\ttfid__min_df: 5\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfid',  TfidfVectorizer(stop_words='english')),\n",
    "    ('lr',LogisticRegression())\n",
    "])\n",
    "parameters = {\n",
    "    'tfid__min_df': (5,10),\n",
    "     'lr__penalty':('l1','l2'),\n",
    "    'lr__C':(0.1,0.2,0.05,0.3,0.6)\n",
    "}\n",
    "scoring = {'auc': 'roc_auc', 'accuracy':'accuracy', 'average_precision': 'average_precision', \n",
    "           'precision':'precision', 'recall':'recall','f1':'f1'}\n",
    "grid_search_tf_idf = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1,scoring=scoring,refit='f1')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "grid_search_tf_idf.fit(train_X_subsample.ravel(), train_y_subsample)\n",
    "print(\"Best score: %0.3f\" % grid_search_tf_idf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_tf_idf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf model Performance with best parameters\n",
      "-------test_accuracy------\n",
      "0.6956965541819827\n",
      "-------test_average_precision------\n",
      "0.7319059514580518\n",
      "-------test_f1------\n",
      "0.7105033377491367\n",
      "-------recall------\n",
      "0.7468256613981399\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(stop_words='english',min_df=5),LogisticRegression(C=0.6,penalty='l2'))\n",
    "    \n",
    "scores = cross_validate(pipeline,\n",
    "                        train_X_subsample.ravel(), train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Tf-idf model Performance with best parameters\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different parameter set does not change the accuracy of the model much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf vectorizer model perform best with the parameter set <br/>\n",
    "Best parameters set:<br/>\n",
    "lr__C: 0.2 <br/>\n",
    "tfid__min_df: 5 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Charcater n gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next approach would be to use character n gram instead of using word ngram or tf-idf vectorizer. We will tune the following parameters for char n gram now. <br/>\n",
    "As done previousluy we will remove the stop words and introduce a L2 penalty on the data\n",
    "\n",
    "1. ngram_range <br/>\n",
    "2. C <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid serach cv for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'lr']\n",
      "parameters:\n",
      "{'vect__ngram_range': ((2, 3), (1, 3)), 'lr__C': (0.2, 0.05)}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 26.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.709\n",
      "Best parameters set:\n",
      "\tlr__C: 0.2\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english',min_df=5,analyzer=\"char_wb\")),\n",
    "    ('lr',LogisticRegression(penalty='l2'))\n",
    "    \n",
    "])\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((2, 3),(1,3)),\n",
    "    'lr__C':(0.2,0.05)\n",
    "}\n",
    "\n",
    "scoring = {'auc': 'roc_auc', 'accuracy':'accuracy', 'average_precision': 'average_precision', \n",
    "           'precision':'precision', 'recall':'recall','f1':'f1'}\n",
    "grid_search_char = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1,scoring=scoring,refit='f1')\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "grid_search_char.fit(train_X_subsample.ravel(), train_y_subsample)\n",
    "print(\"Best score: %0.3f\" % grid_search_char.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_char.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cross validation on best parameters obtained above for char n gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-ngram model Performance with best parameters\n",
      "-------test_accuracy------\n",
      "0.7083629453975749\n",
      "-------test_average_precision------\n",
      "0.757138022180604\n",
      "-------test_f1------\n",
      "0.7321218569996074\n",
      "-------recall------\n",
      "0.7970588367933058\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(stop_words='english',min_df=5,analyzer=\"char_wb\",ngram_range=(1,3)),LogisticRegression(C=0.2,penalty='l2'))\n",
    "    \n",
    "scores = cross_validate(pipeline,\n",
    "                        train_X_subsample.ravel(), train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Char-ngram model Performance with best parameters\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using char n gram gives the best results by far:<br/>\n",
    "1. precision and f1 score increased significantly\n",
    "2. best over all accuracy achieved\n",
    "3. Recall remains the high and similar to other models used above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the above approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to combine the approaches char n gram and tf-idf tuned above to see how our model performs when both the methods are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations of above model performance with best parameters\n",
      "-------test_accuracy------\n",
      "0.7034971739898499\n",
      "-------test_average_precision------\n",
      "0.7513355709056339\n",
      "-------test_f1------\n",
      "0.7082026785256078\n",
      "-------recall------\n",
      "0.7196236986135749\n"
     ]
    }
   ],
   "source": [
    "# combine char n gram and tf-idf \n",
    "\n",
    "pipeline = make_pipeline(CountVectorizer(stop_words='english',min_df=5,analyzer=\"char_wb\",ngram_range=(1,3)),TfidfTransformer(),LogisticRegression(C=0.2,penalty='l2'))\n",
    "    \n",
    "scores = cross_validate(pipeline,\n",
    "                        train_X_subsample.ravel(), train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Combinations of above model performance with best parameters\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that the combination of char n gram and tf-idf does not perform that well, as compared to when they are used individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Extract other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the following new features: from our data\n",
    "1. Length of document\n",
    "2. No of hyperlinks in a document\n",
    "3. No of exclaimation mark in the document\n",
    "4. No of dots in the document\n",
    "5. If the document contain a HTML tag\n",
    "6. No of capital letters in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"I always think of it as a part of our bodies failing to adapt to modern civilization fast enough.\\r\\n\\r\\nThousands of years ago, we'd be exhausted from a day of hard labor, nothing on our minds but waking up to work the next day.\\r\\n\\r\\nNow, some of us may still feel that way, but in our current society we're conditioned to constantly check our phones throughout the day and at night, an endless waterfall of content and alerts that our brain tries to juggle. \\r\\n\\r\\nAnd when we drift off, alone with our thoughts, theres a seemingly endless stream of relationships and previous encounters popping into our head, coupled with a swathe of problems so far in the future we may never face them, but can't help think of them.\"],\n",
       "       [\"Too bad it wasn't made from marble.  Marble is pretty exciting right now. \"],\n",
       "       [\"Did you and the team pick this mission or did you get assigned to it? Was this y'alls life long passion to study the Moon?\"],\n",
       "       ...,\n",
       "       [\"What's scarier, dehumanizing objectifying materialism that is legal and not reported as abuse or dehumanizing objectifying materialism that is illegal and seen as abuse. Morality doesn't depend on legality. \"],\n",
       "       [\"Eventually they're going to use crispr to make pig-human chimeras \"],\n",
       "       ['So india needs to make more saffron']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always think of it as a part of our bodies f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Too bad it wasn't made from marble.  Marble is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did you and the team pick this mission or did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell that to our four failed cycles. Only one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I'm at home by myself with my 2 year old ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  I always think of it as a part of our bodies f...\n",
       "1  Too bad it wasn't made from marble.  Marble is...\n",
       "2  Did you and the team pick this mission or did ...\n",
       "3  Tell that to our four failed cycles. Only one ...\n",
       "4  When I'm at home by myself with my 2 year old ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_subsample_df = pd.DataFrame({'body':train_X_subsample[:,0]})\n",
    "train_X_subsample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new features as new columns to our under sampled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create new features\n",
    "\n",
    "train_X_subsample_df['length'] = train_X_subsample_df['body'].apply(lambda x: (len(x)))\n",
    "train_X_subsample_df['totalHyperlink'] = train_X_subsample_df['body'].str.count('http')\n",
    "train_X_subsample_df['totalExclaimation'] = train_X_subsample_df['body'].str.count('!')\n",
    "train_X_subsample_df['totalDots'] = train_X_subsample_df['body'].str.count('.')\n",
    "train_X_subsample_df['totalUpperCase'] = train_X_subsample_df['body'].apply(lambda x: sum(map(str.isupper, x)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>length</th>\n",
       "      <th>totalHyperlink</th>\n",
       "      <th>totalExclaimation</th>\n",
       "      <th>totalDots</th>\n",
       "      <th>totalUpperCase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always think of it as a part of our bodies f...</td>\n",
       "      <td>712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Too bad it wasn't made from marble.  Marble is...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did you and the team pick this mission or did ...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell that to our four failed cycles. Only one ...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I'm at home by myself with my 2 year old ...</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  length  totalHyperlink  \\\n",
       "0  I always think of it as a part of our bodies f...     712               0   \n",
       "1  Too bad it wasn't made from marble.  Marble is...      74               0   \n",
       "2  Did you and the team pick this mission or did ...     122               0   \n",
       "3  Tell that to our four failed cycles. Only one ...      67               0   \n",
       "4  When I'm at home by myself with my 2 year old ...     343               0   \n",
       "\n",
       "   totalExclaimation  totalDots  totalUpperCase  \n",
       "0                  0        706               4  \n",
       "1                  0         74               2  \n",
       "2                  0        122               3  \n",
       "3                  0         67               2  \n",
       "4                  0        343               8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_subsample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer for word n gram \n",
    "new_count_vect = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5)\n",
    "new_tf_idf_vect = TfidfVectorizer(stop_words='english',min_df=5)\n",
    "\n",
    "# Tf-idf Vectorizer\n",
    "X_count_vect = new_count_vect.fit_transform(train_X_subsample_df['body'].ravel())\n",
    "X_tf_idf_vect = new_tf_idf_vect.fit_transform(train_X_subsample_df['body'].ravel())\n",
    "\n",
    "# char n gram vectorizer\n",
    "new_charc_vect = CountVectorizer(stop_words='english',ngram_range=(1,3),min_df=5,analyzer=\"char_wb\")\n",
    "X_char_vect = new_charc_vect.fit_transform(train_X_subsample_df['body'].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  New features from + Tf-idf features + count vectorizer word n gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_tfidf_vect = sparse.hstack((X_count_vect,X_tf_idf_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeaturesMat = train_X_subsample_df[['length','totalHyperlink','totalExclaimation','totalDots','totalUpperCase']].as_matrix()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129476, 87093)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge new features to our vectorized data\n",
    "new_mat1 = sparse.hstack((X_count_tfidf_vect,newFeaturesMat))\n",
    "new_mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model Performance with best parameters and new features\n",
      "-------test_accuracy------\n",
      "0.6957428921918998\n",
      "-------test_average_precision------\n",
      "0.7202117224627218\n",
      "-------test_f1------\n",
      "0.7204302274873815\n",
      "-------recall------\n",
      "0.7841143900780919\n"
     ]
    }
   ],
   "source": [
    "#  run cross validation to test the model with new features\n",
    "scores = cross_validate(LogisticRegression(C=0.2,penalty='l2'),\n",
    "                        new_mat1, train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Logistic regression model Performance with best parameters and new features\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new features and Tf-idf features + count vectorizer word n gram features, the model does approximately the same as the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. New Features + Char n gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mat = sparse.hstack((X_char_vect,newFeaturesMat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model Performance with best parameters and new features\n",
      "-------test_accuracy------\n",
      "0.70845559397729\n",
      "-------test_average_precision------\n",
      "0.7566988374354118\n",
      "-------test_f1------\n",
      "0.7322642757393277\n",
      "-------recall------\n",
      "0.797367859063921\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(LogisticRegression(C=0.2,penalty='l2'),\n",
    "                        new_mat, train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Logistic regression model Performance with best parameters and new features\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features + char n gram features work similar to only char n gram features and there is no improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. New features +  Tf-idf features + count vectorizer word n gram features + char n gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model Performance with best parameters and new features\n",
      "-------test_accuracy------\n",
      "0.7133985788977038\n",
      "-------test_average_precision------\n",
      "0.7639181696232565\n",
      "-------test_f1------\n",
      "0.7350176974274371\n",
      "-------recall------\n",
      "0.7950043891067118\n"
     ]
    }
   ],
   "source": [
    "#  char n gram + tf-idf + word n gram\n",
    "new_mat2 = sparse.hstack((new_mat1,X_char_vect))\n",
    "scores = cross_validate(LogisticRegression(C=0.2,penalty='l2'),\n",
    "                        new_mat2, train_y_subsample, cv=5,\n",
    "                        scoring=('accuracy','average_precision','recall','f1'))\n",
    "print(\"Logistic regression model Performance with best parameters and new features\")\n",
    "print(\"-------test_accuracy------\\n\"+str(scores['test_accuracy'].mean()))\n",
    "print(\"-------test_average_precision------\\n\"+str(scores['test_average_precision'].mean()))\n",
    "print(\"-------test_f1------\\n\"+str(scores['test_f1'].mean()))\n",
    "print(\"-------recall------\\n\"+str(scores['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model with new features and Tf-id, char-n gram, word n gram vectorization works by far the best and guves the highestaccuracy,precision,recall and f1 score. We can say model imporved by adding new features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use a pretrained word-embedding (word2vec) from genism instead of the bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pre-trained word2vec\n",
    "from gensim import models\n",
    "\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    '../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "word2vecmodel = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_X_subsample, train_y_subsample, stratify=train_y_subsample, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to remove None documents after transformation\n",
    "def get_valid_docs(w2v_docs,y):\n",
    "    indexList = []\n",
    "    validDocs = []\n",
    "    valid_y = []\n",
    "    for i,doc in enumerate(w2v_docs):\n",
    "        if(len(doc)==0):\n",
    "            indexList.append(i)\n",
    "        else:\n",
    "            validDocs.append(doc)\n",
    "            valid_y.append(y[i])\n",
    "    return validDocs,indexList,valid_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our training data to vector using pre trained word2Vec model\n",
    "\n",
    "alldocs = X_train.ravel()\n",
    "y = y_train\n",
    "\n",
    "vect_w2v = CountVectorizer(vocabulary=word2vecmodel.index2word)\n",
    "w2v_docs = vect_w2v.inverse_transform(vect_w2v.transform(alldocs))\n",
    "\n",
    "# There are some docs which do not have mapping to word embedding and become None on transformation\n",
    "# Removing those documents and filtering out only the valid docs \n",
    "valid_w2v_docs,indexList,valid_y = get_valid_docs(w2v_docs,y)\n",
    "valid_X = np.vstack([np.mean(word2vecmodel[doc], axis=0) for doc in valid_w2v_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6729283020423305"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_w2v = LogisticRegression(C=0.2).fit(valid_X, valid_y)\n",
    "print('Training Score')\n",
    "lr_w2v.score(valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation data to vector using pre trained word2Vec model\n",
    "\n",
    "alldocs = X_val.ravel()\n",
    "y = y_val\n",
    "\n",
    "vect_w2v = CountVectorizer(vocabulary=word2vecmodel.index2word)\n",
    "w2v_docs = vect_w2v.inverse_transform(vect_w2v.transform(alldocs))\n",
    "\n",
    "# There are some docs which do not have mapping to word embedding and become None on transformation\n",
    "# Removing those documents and filtering out only the valid docs \n",
    "\n",
    "valid_w2v_docs,indexList,valid_y_val = get_valid_docs(w2v_docs,y)\n",
    "valid_X_val = np.vstack([np.mean(word2vecmodel[doc], axis=0) for doc in valid_w2v_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Embedding, word2vec Performance on validation set\n",
      "------validation_set_score------\n",
      "0.668233402393395\n",
      "-------validation_set_roc------\n",
      "0.668237619554811\n",
      "-------validation_set_precision------\n",
      "0.66502642609805\n",
      "-------validation_set_recall------\n",
      "0.6773295384234624\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Word Embedding, word2vec Performance on validation set\")\n",
    "print(\"------validation_set_score------\\n\"+str(lr_w2v.score(valid_X_val,valid_y_val)))\n",
    "print(\"-------validation_set_roc------\\n\"+str(roc_auc_score(valid_y_val,lr_w2v.predict(valid_X_val))))\n",
    "print(\"-------validation_set_precision------\\n\"+str(precision_score(valid_y_val,lr_w2v.predict(valid_X_val))))\n",
    "print(\"-------validation_set_recall------\\n\"+str(recall_score(valid_y_val,lr_w2v.predict(valid_X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model does not seem to perform any better, It gives rather poor scores as compared to other approaches followed in Task1 But the over all accuracy is not significantly low and this can be considered as a good base line model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
